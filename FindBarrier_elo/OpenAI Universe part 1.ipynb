{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with OpenAI gym\n",
    " \n",
    "The OpenAI gym environment is one of the most fun ways to learn more about machine learning. Especially reinforcement learning and neural networks can be applied perfectly to the benchmark and Atari games collection that is included. Every environment has multiple featured solutions, and often you can find a writeup on how to achieve the same score. By looking at others approaches and ideas you can improve yourself quickly in a fun way. \n",
    " \n",
    "I noticed that getting started with Gym can be a bit difficult. Although there are many tutorials for algorithms online, the first step is understanding the programming environment in which you are working. To easy new people into this environment I decided to make a small tutorial with a docker container and a jupyter notebook. \n",
    "\n",
    "![cartwheel part 1](http://www.pinchofintelligence.com/wp-content/uploads/2017/07/cartwheel-neural-part1-e1500278401585.png)\n",
    " \n",
    "### What you need\n",
    "Before you get started, [install Docker](https://docs.docker.com/engine/installation/#supported-platforms). Docker is a tool that lets you run virtual machines on your computer. I created an \"image\" that contains several things you want to have: tensorflow, the gym environment, numpy, opencv, and some other useful tools. \n",
    "\n",
    "After you installed Docker, run the following command to download my prepared docker image: \n",
    "\n",
    "```\n",
    "docker run -p 8888:8888 rmeertens/tensorflowgym\n",
    "```\n",
    " \n",
    " \n",
    "In your browser, navigate to: localhost:8888 and open the OpenAI Universe notebook in the TRADR folder.  \n",
    " \n",
    "### Play a game yourself\n",
    "Let's start by playing the cartpole game ourselves. You control a bar that has a pole on it. The goal of the \"game\" is to keep the bar upright as long as possible. There are two actions you can perform in this game: give a force to the left, or give a force to the right. To play this game manually, execute the first part of the code. \n",
    "\n",
    "By clicking left and right you apply a force, and you see the new state. Note that I programmed the game to automatically reset when you \"lost\" the game. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import gym\n",
    "\n",
    "from matplotlib import animation\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "\n",
    "\n",
    "\n",
    "def leftclicked(something):\n",
    "    \"\"\" Apply a force to the left of the cart\"\"\"\n",
    "    onclick(0)\n",
    "\n",
    "def rightclicked(something):\n",
    "    \"\"\" Apply a force to the right of the cart\"\"\"\n",
    "    onclick(1)\n",
    "    \n",
    "def display_buttons():\n",
    "    \"\"\" Display the buttons you can use to apply a force to the cart \"\"\"\n",
    "    left = widgets.Button(description=\"<\")\n",
    "    right = widgets.Button(description=\">\")\n",
    "    display(left, right)\n",
    "    \n",
    "    left.on_click(leftclicked)\n",
    "    right.on_click(rightclicked)\n",
    "\n",
    "# Create the environment and display the initial state\n",
    "env = gym.make('CartPole-v0')\n",
    "observation = env.reset()\n",
    "firstframe = env.render(mode = 'rgb_array')\n",
    "fig,ax = plt.subplots()\n",
    "im = ax.imshow(firstframe) \n",
    "\n",
    "# Show the buttons to control the cart\n",
    "display_buttons()\n",
    "\n",
    "\n",
    "# Function that defines what happens when you click one of the buttons\n",
    "frames = []\n",
    "def onclick(action):\n",
    "    global frames\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    frame = env.render(mode = 'rgb_array')\n",
    "    im.set_data(frame)\n",
    "    frames.append(frame)\n",
    "    if done:\n",
    "        env.reset()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![My manual play](http://www.pinchofintelligence.com/wp-content/uploads/2017/07/manualplay.gif)\n",
    "\n",
    "\n",
    "### Replay \n",
    "Now that you toyed around you probably want to see a replay. Every button click we saved the state of the game, which you can display in your browser: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_frames_as_gif(frames, filename_gif = None):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    if filename_gif: \n",
    "        anim.save(filename_gif, writer = 'imagemagick', fps=20)\n",
    "    display(display_animation(anim, default_mode='loop'))\n",
    "\n",
    "display_frames_as_gif(frames, filename_gif=\"manualplay.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Representation\n",
    "The cartpole environment is described on the [OpenAI website](https://gym.openai.com/envs/CartPole-v0). The values in the observation parameter show position (x), velocity (x_dot), angle (theta), and angular velocity (theta_dot). If the pole has an angle of more than 15 degrees, or the cart moves more than 2.4 units from the center, the game is \"over\". The environment can then be reset by calling env.reset(). \n",
    "\n",
    "### Start learning\n",
    "This blogpost would be incomplete without a simple \"learning\" mechanism. Kevin Frans made a great blogpost about simple algorithms you can apply on this problem: http://kvfrans.com/simple-algoritms-for-solving-cartpole/. \n",
    "\n",
    "The simplest one to implement is his random search algorithm. By multiplying parameters with the observation parameters the cart either decides to apply the force left or right. Now the question is: what are the best parameters? Random search defines them at random, sees how long the cart lasts with those parameters, and remembers the best parameters it found. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_episode(env, parameters):  \n",
    "    \"\"\"Runs the env for a certain amount of steps with the given parameters. Returns the reward obtained\"\"\"\n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in xrange(200):\n",
    "        action = 0 if np.matmul(parameters,observation) < 0 else 1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward\n",
    "\n",
    "# Random search: try random parameters between -1 and 1, see how long the game lasts with those parameters\n",
    "bestparams = None  \n",
    "bestreward = 0  \n",
    "for _ in xrange(10000):  \n",
    "    parameters = np.random.rand(4) * 2 - 1\n",
    "    reward = run_episode(env,parameters)\n",
    "    if reward > bestreward:\n",
    "        bestreward = reward\n",
    "        bestparams = parameters\n",
    "        # considered solved if the agent lasts 200 timesteps\n",
    "        if reward == 200:\n",
    "            break\n",
    "            \n",
    "def show_episode(env, parameters):  \n",
    "    \"\"\" Records the frames of the environment obtained using the given parameters... Returns RGB frames\"\"\"\n",
    "    observation = env.reset()\n",
    "    firstframe = env.render(mode = 'rgb_array')\n",
    "    frames = [firstframe]\n",
    "    \n",
    "    for _ in xrange(200):\n",
    "        action = 0 if np.matmul(parameters,observation) < 0 else 1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        frame = env.render(mode = 'rgb_array')\n",
    "        frames.append(frame)\n",
    "        if done:\n",
    "            break\n",
    "    return frames\n",
    "\n",
    "frames = show_episode(env, bestparams)\n",
    "display_frames_as_gif(frames, filename_gif=\"bestresultrandom.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![My best random result](http://www.pinchofintelligence.com/wp-content/uploads/2017/07/bestresultrandom.gif)\n",
    "\n",
    "### Exercises\n",
    "The next step is to play and learn yourself. Here are some suggestions: \n",
    "- Continue with the tutorial Kevin Frans made: http://kvfrans.com/simple-algoritms-for-solving-cartpole/\n",
    "- Upload and share your results. Compare how well either the random algorithm works, or how well the algorithm you implemented yourself works compared to others. How you can do this can be found on this page: https://gym.openai.com/docs#recording-and-uploading-results under the heading \"Recording and uploading results\"\n",
    "- Take a look at the other environments: https://gym.openai.com/envs . If you can solve the cartpole environment you can surely also solve the Pendulum problem (note that you do have to adjust your algorithm, as this one only has 3 variables in its observation). \n",
    "\n",
    "### Conclusion\n",
    "Congratulations! You made your first autonomous pole-balancer in the OpenAI gym environment. Now that this works it is time to either improve your algorithm, or start playing around with different environments. This Jupyter notebook skips a lot of basic knowledge about what you are actually doing, there is a great writeup about that on the [OpenAI site](https://gym.openai.com/docs). \n",
    "\n",
    "Unless you decided to make your own algorithm as an exercise you will not have done a lot of machine learning this tutorial (I don't consider finding random parameters \"learning\"). Next session we will take a look at deep q networks: neural networks that predict the reward of each action. \n",
    "\n",
    "\n",
    "### Acknowledgments \n",
    "This blogpost is the first part of my TRADR summerschool workshop on using human input in reinforcement learning algorithms. More information can be found [on their homepage](https://sites.google.com/view/tradr/home)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
